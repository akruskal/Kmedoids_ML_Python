{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/HDAT9500Banner.PNG)\n",
    "<br>\n",
    "\n",
    "# Chapter 7 Assessment: k-medoids clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####################################################################################\n",
    "\n",
    "Double-click to write down your name and surname.\n",
    "\n",
    "**Name:**\n",
    "\n",
    "\n",
    "**Surname:**\n",
    "\n",
    "**Honour Pledge** <p>\n",
    "    \n",
    "    \n",
    "Declaration: <p>\n",
    "    \n",
    "    \n",
    "I declare that this assessment item is my own work, except where acknowledged, and has not been submitted for academic credit elsewhere or previously, or produced independently of this course (e.g. for a third party such as your place of employment) and acknowledge that the assessor of this item may, for the purpose of assessing this item: \n",
    "\n",
    "    a. Reproduce this assessment item and provide a copy to another member of the University; and/or \n",
    "    b. Communicate a copy of this assessment item to a plagiarism checking service (which may then retain a copy of the assessment item on its database for the purpose of future plagiarism checking). \n",
    "\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assessment we will look at a clustering algorithm called k-medoids, an algorithm similar to k-means. An introduction to k-medoids can be found here: https://en.wikipedia.org/wiki/K-medoids . \n",
    "\n",
    "In k-means the center of a cluster is given by the cluster average or centroid. In k-medoids the center of a cluster is given by a specific data point (the [medoid](https://en.wikipedia.org/wiki/Medoid)). \n",
    "\n",
    "K-means minimises the sum of squared Euclidean distances between the elements of a cluster and the cluster's centroid. K-medoids minimises the sum of distances between the elements of a cluster and the cluster's medoid. \n",
    "\n",
    "K-medoids can make use of arbitrary distance functions. <font color=red>In this assessment, we always use the [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry) as a distance function for k-medoids clustering.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (10 Points)\n",
    "The Manhattan distance between two observations (comprising multiple variables) is more meaningful than the Euclidean distance when the variables are of a specific kind, which one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (20 Points, 5 for each question)\n",
    "Consider the following synthetic data (__data1__), generated from two clusters with 500 observations each and 2 observed variables x and y. The true labels (and colors in the plot) are for illustration only, remember that in unsupervised learning we generally don't have this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "c1 = np.random.multivariate_normal([0,0], 5*np.eye(2), 500)\n",
    "c2 = np.random.multivariate_normal([10,0], 5*np.eye(2), 500)\n",
    "data1 = np.vstack([c1, c2])\n",
    "\n",
    "# plot\n",
    "idx = np.random.choice(1000, (1000,), replace=False) #random plotting order\n",
    "color = np.hstack([np.zeros(500), np.ones(500)])\n",
    "plt.scatter(data1[idx,0], data1[idx,1], c=color[idx])\n",
    "plt.xlim([-10,20])\n",
    "plt.ylim([-10,10])\n",
    "plt.xlabel(\"x-value\")\n",
    "plt.ylabel(\"y-value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply k-means with 2 clusters, 20 random initialisations, random_state=42 to this data. Are results as expected? Where is the border (in terms of x-coordinate) between the two clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans1 = ### TO-DO! ###\n",
    "### TO-DO! ###\n",
    "\n",
    "plt.scatter(data1[:,0], data1[:,1], c=kmeans1.labels_)\n",
    "plt.xlim([-10,20])\n",
    "plt.ylim([-10,10])\n",
    "plt.xlabel(\"x-value\")\n",
    "plt.ylabel(\"y-value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a similar synthetic dataset (__data2__) and apply k-means (same parameters as before). Are results as expected? Where is the computed border (in terms of x-coordinate) between the two clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "c1 = np.random.multivariate_normal([0,0], 5*np.eye(2), 500)\n",
    "c2 = np.random.multivariate_normal([np.log(10),0], [[1,0],[0,5]], 500)\n",
    "c2[:,0] = np.exp(c2[:,0])\n",
    "\n",
    "data2 = np.vstack([c1, c2])\n",
    "\n",
    "# plot\n",
    "idx = np.random.choice(1000, (1000,), replace=False) #random plotting order\n",
    "color = np.hstack([np.zeros(500), np.ones(500)])\n",
    "plt.scatter(data2[idx,0], data2[idx,1], c=color[idx])\n",
    "plt.xlim([-10,60])\n",
    "plt.ylim([-10,10])\n",
    "plt.xlabel(\"x-value\")\n",
    "plt.ylabel(\"y-value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans2 = ### TO-DO! ###\n",
    "### TO-DO! ###\n",
    "label_kmeans2 = kmeans2.labels_\n",
    "\n",
    "plt.scatter(data2[:,0], data2[:,1], c=label_kmeans2)\n",
    "plt.xlim([-10,60])\n",
    "plt.ylim([-10,10])\n",
    "plt.xlabel(\"x-value\")\n",
    "plt.ylabel(\"y-value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (50 Points)\n",
    "Implement the k-medoids algorithm using the Manhattan distance and apply it to __data2__ (find two clusters as before). You can use a greedy approach and try out every pair of medoids or use a more efficient implementation. Are results improved compared with k-means? Where is the computed border (in terms of x-coordinate) between the two clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def k_medoids_2(data):\n",
    "    '''\n",
    "    Implement k-medoids algorithm for subdividing the data into two clusters\n",
    "    Inputs:\n",
    "        data: data array with observations as rows and features as columns\n",
    "    Outputs:\n",
    "        label: the label assigned to each observation (a binary vector containing zeros and ones)\n",
    "        medoid1: the medoid of the first cluster\n",
    "        medoid2: the medoid of the second cluster\n",
    "    '''\n",
    "    \n",
    "    ### TO-DO! ###\n",
    "    \n",
    "    return label, medoid1, medoid2\n",
    "\n",
    "label_kmedoids2, medoid1, medoid2 = k_medoids_2(data2)\n",
    "print(medoid1)\n",
    "print(medoid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-medoids clustering\n",
    "plt.scatter(data2[:,0], data2[:,1], c=label_kmedoids2)\n",
    "plt.xlim([-10,60])\n",
    "plt.ylim([-10,10])\n",
    "plt.xlabel(\"x-value\")\n",
    "plt.ylabel(\"y-value\")\n",
    "plt.show()\n",
    "\n",
    "# k-means clustering\n",
    "plt.scatter(data2[:,0], data2[:,1], c=label_kmeans2)\n",
    "plt.xlim([-10,60])\n",
    "plt.ylim([-10,10])\n",
    "plt.xlabel(\"x-value\")\n",
    "plt.ylabel(\"y-value\")\n",
    "plt.show()\n",
    "\n",
    "# true label\n",
    "idx = np.random.choice(1000, (1000,), replace=False) #random plotting order\n",
    "color = np.hstack([np.zeros(500), np.ones(500)])\n",
    "plt.scatter(data2[idx,0], data2[idx,1], c=color[idx])\n",
    "plt.xlim([-10,60])\n",
    "plt.ylim([-10,10])\n",
    "plt.xlabel(\"x-value\")\n",
    "plt.ylabel(\"y-value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (20 Points)\n",
    "Why are the k-medoids results different from k-means on __data2__? (15 points)\n",
    "\n",
    "_Hint: notice that the x values of the generated clusters have either normal or log-normal distribution. What is their mean and median? How does this relate to the cost function that k-means and k-medoids are trying to minimise?_\n",
    "\n",
    "Do you expect k-means and k-medoids results to differ on __data1__? why? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
