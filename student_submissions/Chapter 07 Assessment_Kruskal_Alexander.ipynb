{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/HDAT9500Banner.PNG)\n",
    "<br>\n",
    "\n",
    "# Assessment Chapter 7\n",
    "\n",
    "## 1.1. Bisecting K-Means\n",
    "\n",
    "K-Means often results in clusters of widely different sizes. In this assessment you are asked to implement an extension to k-means called bisecting k-means. The algorithm proceeds as follows:\n",
    "\n",
    "1. Pick a cluster to split (for this exercise, always pick the largest one)\n",
    "2. Find 2 sub-clusters using the basic k-means algorithm (bisecting step)\n",
    "3. Repeat step 2 for ITER times (for this exercise, set ITER=20) and take the split that minimizes the inertia\n",
    "4. Repeat steps 1, 2 and 3 until the desired number of clusters is reached\n",
    "\n",
    "## 1.2. Tasks\n",
    "\n",
    "1. Implement the bisecting k-means algorithm.\n",
    "2. Apply bisecting k-means to the Breast Cancer Wisconsin (Diagnostic) Data Set using the first 10 numerical features in the dataset as feature vectors (as was done in the practical exercise \"Exercise 7 - PCA\"). Remember to scale the data to have zero mean and unit variance before clustering. Run the algorithm three different times so that the data are divided into: (a) 2 clusters, (b) 5 clusters, (c) 10 clusters.\n",
    "3. Compare the number of observations in each cluster between basic k-means and bisecting k-means when the data are divided into 10 clusters.\n",
    "\n",
    "## 1.2. Aims:\n",
    "\n",
    "1. Gain a better understanding of k-means and clustering algorithms in general.\n",
    "\n",
    "## 1.3. Tips:\n",
    "\n",
    "You are allowed to use any function that was used in the practical exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to cluster standard k means using unscaled\n",
    "\n",
    "def standard_kmeans(X, desired_cluster_number):\n",
    "    #import libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    #scale X\n",
    "    X_scaled = preprocessing.scale(X)\n",
    "\n",
    "    k = desired_cluster_number\n",
    "    kmeans = KMeans(n_clusters=desired_cluster_number, init=\"random\", n_init=20,\n",
    "                     algorithm=\"full\", random_state=1)\n",
    "    y_pred=kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    return(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to cluster bissect k means using unscaled data\n",
    "\n",
    "def bisect_kmeans(X, desired_cluster_number):\n",
    "    #import libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    #initial k means bissect\n",
    "    X_scaled = preprocessing.scale(X) #scale X\n",
    "       \n",
    "    k = 2\n",
    "    kmeans = KMeans(n_clusters=2, init=\"random\", n_init=20,\n",
    "                     algorithm=\"full\", random_state=1)\n",
    "    y_pred=kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    \n",
    "    #initial variables\n",
    "    clusters=2\n",
    "    total_size = np.shape(X_scaled)[0]\n",
    "    \n",
    "    \n",
    "    #if desired clusters > 2 (initial bissect)\n",
    "    while clusters < desired_cluster_number: #loop until desired number of clusters\n",
    "        list_cluster_sizes = [] #reset list every new cluster made\n",
    "                \n",
    "        for e in range (0,clusters): #loop through all current clusters\n",
    "            sum_cluster = int(0) #reset cluster sum every time\n",
    "            for i in range (1,total_size): #count number in each cluster\n",
    "                if y_pred[i]== e:\n",
    "                    sum_cluster += 1\n",
    "            #append size of each cluster, index = cluster number\n",
    "            list_cluster_sizes.append(sum_cluster)\n",
    "            \n",
    "        largest_cluster=list_cluster_sizes.index(max(list_cluster_sizes)) #find index largest (index = cluster)\n",
    "        \n",
    "        print(list_cluster_sizes) #sanity check largest cluster is bissected\n",
    "        \n",
    "        #make largest cluster = 0 and remove 1's so you can recluster using the 0's and 1's bissecting\n",
    "        if largest_cluster == 0:\n",
    "            #kick 1's out\n",
    "            y_pred[y_pred==1] = clusters\n",
    "\n",
    "        elif largest_cluster == 1:\n",
    "            y_pred[y_pred==0]= clusters #0's are new number\n",
    "            y_pred[y_pred==1] = 0 #1's become 0's\n",
    "        else:\n",
    "            y_pred[y_pred==0]= clusters #0 = new\n",
    "            y_pred[y_pred==largest_cluster]= 0 #largest becomes 0\n",
    "            y_pred[y_pred==1]= largest_cluster #1's become the old largest cluster value\n",
    "        \n",
    "        #fit model using 0 which is largest cluster\n",
    "        '''rescales biggest cluster'''\n",
    "        #X_largest_Cluster = X[y_pred==0] #select only largest cluster from unscaled data\n",
    "        #X_largest_Cluster_scaled = preprocessing.scale(X_largest_Cluster) #scale\n",
    "        #y_pred_temp=kmeans.fit_predict(X_largest_Cluster_scaled) #bissect largest cluster into temp y_pred\n",
    "        '''does not rescale biggest cluster'''\n",
    "        X_largest_Cluster = X_scaled[y_pred==0]\n",
    "        y_pred_temp=kmeans.fit_predict(X_largest_Cluster) #no rescaling\n",
    "        \n",
    "        #input split cluster(0's and 1's) in main y_pred where they were 0\n",
    "        index_y_pred_temp = 0 #track index of y_temp used\n",
    "        for i in range (1,total_size):\n",
    "            if y_pred[i]==0:\n",
    "                y_pred[i]=y_pred_temp[index_y_pred_temp]\n",
    "                index_y_pred_temp +=1\n",
    "        \n",
    "        \n",
    "        clusters += 1 #counter for number of clusters\n",
    "   \n",
    "    return(y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "bcw = pd.read_csv(\"C:/Users/akrus/Desktop/ML&DM/chapter07-akruskal/data/breast-cancer-wisconsin-data/data.csv\", sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n",
       "count   5.690000e+02       569   569.000000    569.000000      569.000000   \n",
       "unique           NaN         2          NaN           NaN             NaN   \n",
       "top              NaN         B          NaN           NaN             NaN   \n",
       "freq             NaN       357          NaN           NaN             NaN   \n",
       "mean    3.037183e+07       NaN    14.127292     19.289649       91.969033   \n",
       "std     1.250206e+08       NaN     3.524049      4.301036       24.298981   \n",
       "min     8.670000e+03       NaN     6.981000      9.710000       43.790000   \n",
       "25%     8.692180e+05       NaN    11.700000     16.170000       75.170000   \n",
       "50%     9.060240e+05       NaN    13.370000     18.840000       86.240000   \n",
       "75%     8.813129e+06       NaN    15.780000     21.800000      104.100000   \n",
       "max     9.113205e+08       NaN    28.110000     39.280000      188.500000   \n",
       "\n",
       "          area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "count    569.000000       569.000000        569.000000      569.000000   \n",
       "unique          NaN              NaN               NaN             NaN   \n",
       "top             NaN              NaN               NaN             NaN   \n",
       "freq            NaN              NaN               NaN             NaN   \n",
       "mean     654.889104         0.096360          0.104341        0.088799   \n",
       "std      351.914129         0.014064          0.052813        0.079720   \n",
       "min      143.500000         0.052630          0.019380        0.000000   \n",
       "25%      420.300000         0.086370          0.064920        0.029560   \n",
       "50%      551.100000         0.095870          0.092630        0.061540   \n",
       "75%      782.700000         0.105300          0.130400        0.130700   \n",
       "max     2501.000000         0.163400          0.345400        0.426800   \n",
       "\n",
       "        concave points_mean           ...             radius_worst  \\\n",
       "count            569.000000           ...               569.000000   \n",
       "unique                  NaN           ...                      NaN   \n",
       "top                     NaN           ...                      NaN   \n",
       "freq                    NaN           ...                      NaN   \n",
       "mean               0.048919           ...                16.269190   \n",
       "std                0.038803           ...                 4.833242   \n",
       "min                0.000000           ...                 7.930000   \n",
       "25%                0.020310           ...                13.010000   \n",
       "50%                0.033500           ...                14.970000   \n",
       "75%                0.074000           ...                18.790000   \n",
       "max                0.201200           ...                36.040000   \n",
       "\n",
       "        texture_worst  perimeter_worst   area_worst  smoothness_worst  \\\n",
       "count      569.000000       569.000000   569.000000        569.000000   \n",
       "unique            NaN              NaN          NaN               NaN   \n",
       "top               NaN              NaN          NaN               NaN   \n",
       "freq              NaN              NaN          NaN               NaN   \n",
       "mean        25.677223       107.261213   880.583128          0.132369   \n",
       "std          6.146258        33.602542   569.356993          0.022832   \n",
       "min         12.020000        50.410000   185.200000          0.071170   \n",
       "25%         21.080000        84.110000   515.300000          0.116600   \n",
       "50%         25.410000        97.660000   686.500000          0.131300   \n",
       "75%         29.720000       125.400000  1084.000000          0.146000   \n",
       "max         49.540000       251.200000  4254.000000          0.222600   \n",
       "\n",
       "        compactness_worst  concavity_worst  concave points_worst  \\\n",
       "count          569.000000       569.000000            569.000000   \n",
       "unique                NaN              NaN                   NaN   \n",
       "top                   NaN              NaN                   NaN   \n",
       "freq                  NaN              NaN                   NaN   \n",
       "mean             0.254265         0.272188              0.114606   \n",
       "std              0.157336         0.208624              0.065732   \n",
       "min              0.027290         0.000000              0.000000   \n",
       "25%              0.147200         0.114500              0.064930   \n",
       "50%              0.211900         0.226700              0.099930   \n",
       "75%              0.339100         0.382900              0.161400   \n",
       "max              1.058000         1.252000              0.291000   \n",
       "\n",
       "        symmetry_worst  fractal_dimension_worst  \n",
       "count       569.000000               569.000000  \n",
       "unique             NaN                      NaN  \n",
       "top                NaN                      NaN  \n",
       "freq               NaN                      NaN  \n",
       "mean          0.290076                 0.083946  \n",
       "std           0.061867                 0.018061  \n",
       "min           0.156500                 0.055040  \n",
       "25%           0.250400                 0.071460  \n",
       "50%           0.282200                 0.080040  \n",
       "75%           0.317900                 0.092080  \n",
       "max           0.663800                 0.207500  \n",
       "\n",
       "[11 rows x 32 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data\n",
    "bcw.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
      "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
      "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
      "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
      "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
      "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
      "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
      "       'symmetry_worst', 'fractal_dimension_worst'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  fractal_dimension_mean  \n",
       "count     569.000000              569.000000  \n",
       "mean        0.181162                0.062798  \n",
       "std         0.027414                0.007060  \n",
       "min         0.106000                0.049960  \n",
       "25%         0.161900                0.057700  \n",
       "50%         0.179200                0.061540  \n",
       "75%         0.195700                0.066120  \n",
       "max         0.304000                0.097440  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select parameters - remove ID and diagnosis\n",
    "print(bcw.columns)\n",
    "X = bcw[bcw.columns[2:12]]\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1\n",
      " 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
      " 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "two_cluster_bissect = bisect_kmeans(X, 2) #X = unscaled data\n",
    "print(two_cluster_bissect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1\n",
      " 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
      " 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "two_cluster_standard = standard_kmeans(X, 2) #X = unscaled data\n",
    "print(two_cluster_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168, 400]\n",
      "[153, 247, 168]\n",
      "[158, 89, 168, 153]\n",
      "[0 1 0 0 1 0 1 0 1 1 2 2 1 2 1 1 2 1 1 4 3 3 0 1 0 1 1 1 0 1 0 3 0 1 0 1 3\n",
      " 4 2 3 2 3 0 3 2 0 3 3 3 2 4 4 4 0 2 4 0 0 4 3 3 3 1 3 3 1 3 4 3 4 1 3 0 3\n",
      " 4 2 3 1 1 4 3 3 1 0 3 0 2 0 2 0 2 2 4 4 1 1 3 3 3 3 2 3 4 3 3 0 3 4 1 4 3\n",
      " 3 1 3 3 2 3 1 1 2 4 1 0 3 4 4 2 1 0 1 3 0 0 4 0 2 4 4 0 3 3 2 3 4 4 3 1 2\n",
      " 4 4 3 3 1 3 4 4 1 2 4 4 3 0 0 3 0 2 4 2 0 4 3 2 0 3 4 3 3 1 4 4 0 0 2 4 2\n",
      " 4 2 4 4 4 1 2 4 3 1 4 1 1 0 3 3 0 0 0 3 4 3 2 3 4 1 4 0 0 0 3 3 4 1 0 4 3\n",
      " 3 0 4 4 3 4 2 0 1 2 2 1 4 2 0 0 2 0 4 4 3 2 0 3 4 3 3 3 0 4 0 0 0 3 0 0 1\n",
      " 1 1 2 0 2 0 0 3 2 4 3 4 3 0 3 2 3 4 2 4 4 0 4 0 0 4 4 2 4 3 4 3 2 3 4 4 4\n",
      " 4 4 4 3 1 2 0 3 4 2 4 4 4 4 4 4 4 4 3 4 4 1 3 4 3 0 3 0 4 4 4 4 1 1 1 3 3\n",
      " 4 4 0 3 0 3 0 3 3 3 0 3 3 4 4 4 3 4 0 1 2 4 2 3 4 3 3 4 2 4 2 4 0 0 4 0 0\n",
      " 0 4 1 0 4 3 3 2 4 0 3 4 2 3 4 2 4 4 3 1 3 3 0 1 3 4 3 4 4 4 0 4 4 4 4 3 4\n",
      " 2 1 4 4 3 4 2 2 3 3 0 4 4 4 1 3 2 3 4 3 4 4 4 1 3 1 0 4 3 4 4 4 4 3 0 4 4\n",
      " 0 3 0 4 2 0 2 0 2 3 4 2 2 2 2 2 0 0 2 4 4 2 2 4 0 3 3 2 4 2 3 4 2 4 3 1 4\n",
      " 4 3 4 3 3 4 1 3 2 2 4 0 4 4 2 3 4 0 0 3 0 3 1 0 3 3 3 4 1 4 4 1 4 2 3 1 0\n",
      " 3 3 3 0 4 3 3 3 3 4 3 3 3 3 4 0 3 0 3 3 3 3 3 2 2 2 2 2 3 3 4 2 4 3 2 3 2\n",
      " 2 3 2 2 2 2 2 0 1 0 0 2 0 4]\n"
     ]
    }
   ],
   "source": [
    "five_cluster_bissect = bisect_kmeans(X, 5)\n",
    "print(five_cluster_bissect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 1 0 1 1 2 2 1 2 1 1 2 1 1 4 3 3 0 1 0 1 1 1 0 1 0 3 0 1 0 1 3\n",
      " 4 2 3 2 3 0 3 2 0 3 3 3 2 4 4 4 0 2 4 0 0 4 3 3 3 1 3 3 1 3 4 3 4 1 3 0 3\n",
      " 4 2 3 1 1 4 3 3 1 0 3 0 2 0 2 0 2 2 4 4 1 1 3 3 3 3 2 3 4 3 3 0 3 4 1 4 3\n",
      " 3 1 3 3 2 3 1 1 2 4 1 0 3 4 4 2 1 0 1 3 0 0 4 0 2 4 4 0 3 3 2 3 4 4 3 1 2\n",
      " 4 4 3 3 1 3 4 4 1 2 4 4 3 0 0 3 0 2 4 2 0 4 3 2 0 3 4 3 3 1 4 4 0 0 2 4 2\n",
      " 4 2 4 4 4 1 2 4 3 1 4 1 1 0 3 3 0 0 0 3 4 3 2 3 4 1 4 0 0 0 3 3 4 1 0 4 3\n",
      " 3 0 4 4 3 4 2 0 1 2 2 1 4 2 0 0 2 0 4 4 3 2 0 3 4 3 3 3 0 4 0 0 0 3 0 0 1\n",
      " 1 1 2 0 2 0 0 3 2 4 3 4 3 0 3 2 3 4 2 4 4 0 4 0 0 4 4 2 4 3 4 3 2 3 4 4 4\n",
      " 4 4 4 3 1 2 0 3 4 2 4 4 4 4 4 4 4 4 3 4 4 1 3 4 3 0 3 0 4 4 4 4 1 1 1 3 3\n",
      " 4 4 0 3 0 3 0 3 3 3 0 3 3 4 4 4 3 4 0 1 2 4 2 3 4 3 3 4 2 4 2 4 0 0 4 0 0\n",
      " 0 4 1 0 4 3 3 2 4 0 3 4 2 3 4 2 4 4 3 1 3 3 0 1 3 4 3 4 4 4 0 4 4 4 4 3 4\n",
      " 2 1 4 4 3 4 2 2 3 3 0 4 4 4 1 3 2 3 4 3 4 4 4 1 3 1 0 4 3 4 4 4 4 3 0 4 4\n",
      " 0 3 0 4 2 0 2 0 2 3 4 2 2 2 2 2 0 0 2 4 4 2 2 4 0 3 3 2 4 2 3 4 2 4 3 1 4\n",
      " 4 3 4 3 3 4 1 3 2 2 4 0 4 4 2 3 4 0 0 3 0 3 1 0 3 3 3 4 1 4 4 1 4 2 3 1 0\n",
      " 3 3 3 0 4 3 3 3 3 4 3 3 3 3 4 0 3 0 3 3 3 3 3 2 2 2 2 2 3 3 4 2 4 3 2 3 2\n",
      " 2 3 2 2 2 2 2 0 1 0 0 2 0 4]\n"
     ]
    }
   ],
   "source": [
    "five_cluster_standard = standard_kmeans(X, 5)\n",
    "print(five_cluster_bissect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168, 400]\n",
      "[153, 247, 168]\n",
      "[158, 89, 168, 153]\n",
      "[101, 67, 89, 153, 158]\n",
      "[80, 78, 89, 153, 67, 101]\n",
      "[84, 69, 89, 78, 67, 101, 80]\n",
      "[31, 70, 89, 78, 67, 69, 80, 84]\n",
      "[42, 47, 70, 78, 67, 69, 80, 84, 31]\n",
      "[8 4 8 2 4 8 4 8 4 4 7 7 4 7 4 4 7 4 4 6 5 0 8 4 8 4 4 4 2 4 8 5 2 4 8 4 5\n",
      " 3 9 5 9 5 8 5 7 2 1 5 0 7 3 3 6 2 7 6 2 2 3 0 1 1 4 1 5 4 1 3 5 6 4 5 8 5\n",
      " 6 7 5 4 4 6 1 5 4 2 1 2 7 2 9 2 7 7 3 6 4 4 1 0 0 5 7 0 3 1 1 8 5 3 4 3 0\n",
      " 5 4 5 0 9 0 4 4 7 6 4 8 5 3 6 9 4 8 4 0 8 2 6 2 9 6 6 2 0 0 7 0 6 3 5 4 7\n",
      " 6 3 5 5 4 0 6 6 4 7 6 3 5 8 2 0 2 7 6 7 2 6 0 7 2 0 6 0 5 4 3 3 8 2 7 3 7\n",
      " 6 7 6 6 3 4 9 3 5 4 6 4 4 8 5 1 2 2 2 5 6 1 7 5 3 4 3 8 2 2 5 1 6 4 8 6 5\n",
      " 1 2 3 6 0 6 9 2 4 9 9 4 3 7 8 2 9 2 6 3 5 9 2 1 3 5 1 0 2 6 2 2 2 5 2 2 4\n",
      " 4 4 7 8 7 2 2 1 9 6 5 3 0 2 1 7 5 6 7 3 6 2 6 2 2 3 3 9 3 5 6 5 7 0 6 6 6\n",
      " 3 6 3 1 4 7 8 0 3 9 3 3 3 3 6 3 6 6 1 3 3 4 5 3 0 8 0 2 6 6 3 3 4 4 4 5 1\n",
      " 6 3 8 0 2 1 2 5 1 0 2 0 0 3 6 6 5 3 2 4 7 3 7 5 3 1 0 3 9 6 7 3 8 2 6 2 2\n",
      " 2 6 4 2 6 5 5 9 6 2 5 6 9 5 6 7 6 3 0 4 0 1 8 4 1 3 5 3 3 6 8 6 6 6 3 0 6\n",
      " 9 4 6 3 0 3 7 9 1 1 8 6 6 6 4 5 7 1 3 0 6 3 3 4 5 4 8 6 5 6 6 3 3 0 2 3 6\n",
      " 2 5 2 6 7 2 9 2 9 5 6 9 9 9 9 9 2 2 9 3 3 7 7 3 2 5 1 9 6 9 0 6 7 3 0 4 3\n",
      " 3 5 6 5 5 3 4 0 7 9 3 2 3 3 7 5 3 2 2 5 2 5 4 8 5 5 5 6 4 3 6 4 6 7 1 4 8\n",
      " 5 5 5 2 3 5 0 0 5 6 5 0 0 1 6 2 0 2 5 5 1 1 0 7 9 9 7 9 1 0 3 9 3 1 9 1 9\n",
      " 9 1 9 7 9 9 9 2 4 8 2 7 2 3]\n"
     ]
    }
   ],
   "source": [
    "ten_cluster_bissect = bisect_kmeans(X, 10)\n",
    "print(ten_cluster_bissect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 4 8 2 4 8 4 8 4 4 7 7 4 7 4 4 7 4 4 6 5 0 8 4 8 4 4 4 2 4 8 5 2 4 8 4 5\n",
      " 3 9 5 9 5 8 5 7 2 1 5 0 7 3 3 6 2 7 6 2 2 3 0 1 1 4 1 5 4 1 3 5 6 4 5 8 5\n",
      " 6 7 5 4 4 6 1 5 4 2 1 2 7 2 9 2 7 7 3 6 4 4 1 0 0 5 7 0 3 1 1 8 5 3 4 3 0\n",
      " 5 4 5 0 9 0 4 4 7 6 4 8 5 3 6 9 4 8 4 0 8 2 6 2 9 6 6 2 0 0 7 0 6 3 5 4 7\n",
      " 6 3 5 5 4 0 6 6 4 7 6 3 5 8 2 0 2 7 6 7 2 6 0 7 2 0 6 0 5 4 3 3 8 2 7 3 7\n",
      " 6 7 6 6 3 4 9 3 5 4 6 4 4 8 5 1 2 2 2 5 6 1 7 5 3 4 3 8 2 2 5 1 6 4 8 6 5\n",
      " 1 2 3 6 0 6 9 2 4 9 9 4 3 7 8 2 9 2 6 3 5 9 2 1 3 5 1 0 2 6 2 2 2 5 2 2 4\n",
      " 4 4 7 8 7 2 2 1 9 6 5 3 0 2 1 7 5 6 7 3 6 2 6 2 2 3 3 9 3 5 6 5 7 0 6 6 6\n",
      " 3 6 3 1 4 7 8 0 3 9 3 3 3 3 6 3 6 6 1 3 3 4 5 3 0 8 0 2 6 6 3 3 4 4 4 5 1\n",
      " 6 3 8 0 2 1 2 5 1 0 2 0 0 3 6 6 5 3 2 4 7 3 7 5 3 1 0 3 9 6 7 3 8 2 6 2 2\n",
      " 2 6 4 2 6 5 5 9 6 2 5 6 9 5 6 7 6 3 0 4 0 1 8 4 1 3 5 3 3 6 8 6 6 6 3 0 6\n",
      " 9 4 6 3 0 3 7 9 1 1 8 6 6 6 4 5 7 1 3 0 6 3 3 4 5 4 8 6 5 6 6 3 3 0 2 3 6\n",
      " 2 5 2 6 7 2 9 2 9 5 6 9 9 9 9 9 2 2 9 3 3 7 7 3 2 5 1 9 6 9 0 6 7 3 0 4 3\n",
      " 3 5 6 5 5 3 4 0 7 9 3 2 3 3 7 5 3 2 2 5 2 5 4 8 5 5 5 6 4 3 6 4 6 7 1 4 8\n",
      " 5 5 5 2 3 5 0 0 5 6 5 0 0 1 6 2 0 2 5 5 1 1 0 7 9 9 7 9 1 0 3 9 3 1 9 1 9\n",
      " 9 1 9 7 9 9 9 2 4 8 2 7 2 3]\n"
     ]
    }
   ],
   "source": [
    "ten_cluster_standard = standard_kmeans(X, 10)\n",
    "print(ten_cluster_bissect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two cluster bissect: Counter({1: 400, 0: 169})\n",
      "Two cluster standard: Counter({1: 400, 0: 169})\n",
      "********************************************************************************************************\n",
      "Five cluster bissect: Counter({4: 158, 3: 153, 0: 102, 2: 89, 1: 67})\n",
      "Five cluster standard: Counter({4: 190, 1: 175, 3: 94, 2: 72, 0: 38})\n",
      "********************************************************************************************************\n",
      "Ten cluster bissect: Counter({6: 80, 3: 78, 2: 70, 5: 69, 4: 67, 7: 47, 0: 46, 9: 42, 1: 38, 8: 32})\n",
      "Ten cluster standard: Counter({1: 101, 0: 94, 6: 68, 7: 66, 5: 57, 9: 51, 3: 50, 8: 45, 4: 23, 2: 14})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print('Two cluster bissect:', Counter(two_cluster_bissect))\n",
    "print('Two cluster standard:', Counter(two_cluster_standard))\n",
    "print('********************************************************************************************************')\n",
    "print('Five cluster bissect:',Counter(five_cluster_bissect))\n",
    "print('Five cluster standard:', Counter(five_cluster_standard))\n",
    "print('********************************************************************************************************')\n",
    "print('Ten cluster bissect:',Counter(ten_cluster_bissect))\n",
    "print('Ten cluster standard:', Counter(ten_cluster_standard))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Based on the observation on the counts above, it is clear that standard k means produces more varied cluster sizes than bissect k means."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
